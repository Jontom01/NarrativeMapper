# NarrativeMapper

**Overview:**

NarrativeMapper is a discourse analysis pipeline that uncovers the dominant narratives and emotional tones within Reddit communities.

This project uses the Reddit API to scrape user comments from top posts in specific subreddits, then applies OpenAIâ€™s embedding API (text-embedding-3-large) to convert text into semantic vectors. These embeddings are clustered using UMAP for dimensionality reduction and HDBSCAN for density-based clustering.

For each discovered cluster, the tool:

- Extracts the main talking points using KeyBERT (keyword extraction from BERT embeddings)

- Analyzes the emotional tone using a Hugging Face sentiment classifier

- Outputs structured summaries of the narrative + emotion pairs


**Example Output:**

This example is based off of 1200 r/politics comments from the top 200 posts within the last year (Date of Writing: 2025-04-02)

<details>
<summary>click to view output example</summary>

```python
{
    'subreddit': 'politics',
    'clusters': [
        {
            'label': [
                'trump appear hispanic',
                'trump instinct racist',
                'trump told rally'
            ],
            'tone': 'NEGATIVE',
            'comment_count': 500
        },
        {
            'label': [
                'bannon putin messages',
                'putin focus groups',
                'analytica steve bannon'
            ],
            'tone': 'NEGATIVE',
            'comment_count': 101
        },
        {
            'label': [
                'hilarious offended republicans',
                'offended republicans',
                'insulting republicans'
            ],
            'tone': 'NEGATIVE',
            'comment_count': 74
        },
        {
            'label': [
                'federal abortion',
                'federal abortion clinics',
                'treatment include abortions'
            ],
            'tone': 'NEGATIVE',
            'comment_count': 102
        },
        {
            'label': [
                'vance candidate eventually',
                'walz announced vance',
                'vance candidate'
            ],
            'tone': 'NEUTRAL',
            'comment_count': 64
        },
        {
            'label': [
                'facing harris trump',
                'trump challenging harris',
                'trump debate week'
            ],
            'tone': 'NEGATIVE',
            'comment_count': 120
        },
        {
            'label': [
                'polling shows trump',
                'recent polling wisconsin',
                'trump michigan polling'
            ],
            'tone': 'NEUTRAL',
            'comment_count': 119
        }
    ]
}
```

</details>


**Architecture:**

----------------------------------------------------------------------------------------------------------------------------

Text Data (reddit_scraper.py) ----> CSV Text Data

CSV Text Data ----> Embeddings (embeddings.py) ----> Cluster (clustering.py) ----> Summarize (summarize.py)

----------------------------------------------------------------------------------------------------------------------------

*reddit_scraper.py:*
Used to scrape reddit comments (Reddit PRAW). This data is then turned into a csv file.

*embeddings.py:*
Converts comments into 3072 dimensional vectors (OPEN AI's text-embedding-3-large).

*clustering.py:*
Clusters embedding vectors using UMAP + HDBSCAN. I reduced the dimensions to 50 during reduction (after some attempts and fails). The other variables related to dimension reduction and clustering are dependent on the ammount of vectors (comments).

*summarize.py:*
Extracts keywords (KeyBERT) and semantics (Hugging Face sentiment classifier) for each cluster. I assign the top 3 most likely key phrases generated by KeyBERT to be used in the final labeling of the clusters. Using the sentiment classifier on each comment, I determine a cluster to be "POSITIVE" if there are 2 times more (or greater) positive comments than negative, and vice-versa for clusters that are determined to be "NEGATIVE".
